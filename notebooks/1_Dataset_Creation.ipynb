{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a Finetuning Dataset\n",
    "\n",
    "We will first create a high-quality, instruction-based dataset for finetuning a smaller language model. \n",
    "\n",
    "**Process:**\n",
    "1.  **Load a source document:** We'll use the sample PDF provided in `data/source_documents/`.\n",
    "2.  **Chunk the document:** Break the document into smaller, manageable text chunks.\n",
    "3.  **Use a powerful generator model:** For each chunk, we will prompt a powerful LLM to generate relevant question-and-answer pairs.\n",
    "4.  **Format and Save:** The generated pairs will be structured and saved into a `.jsonl` file, which is a common format for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from: ../data/source_documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2 PDFs => 3 documents (pages).\n",
      "Splitting 3 documents into chunks of size 1000 with overlap 100...\n",
      "Successfully split documents into 13 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open(\"../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f) \n",
    "\n",
    "# Load documents\n",
    "directory_path = \"../\"+config['data']['source_documents_path']\n",
    "print(f\"Loading documents from: {directory_path}\")\n",
    "loader = DirectoryLoader(\n",
    "    directory_path,\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True,\n",
    "    use_multithreading=True\n",
    ")\n",
    "source_docs = loader.load()\n",
    "if not source_docs:\n",
    "    print(f\"Warning: No documents loaded from {directory_path}. Make sure there are PDF files in the directory.\")\n",
    "    source_docs = []\n",
    "else:   print(f\"Successfully loaded {sum(f.endswith('.pdf') for f in os.listdir(directory_path))} PDFs => {len(source_docs)} documents (pages).\")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunk_size = config['data']['chunk_size']\n",
    "chunk_overlap = config['data']['chunk_overlap']\n",
    "print(f\"Splitting {len(source_docs)} documents into chunks of size {chunk_size} with overlap {chunk_overlap}...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "doc_chunks = text_splitter.split_documents(source_docs)\n",
    "print(f\"Successfully split documents into {len(doc_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gemini model...\n",
      "Successfully initialised Gemini for dataset generation.\n"
     ]
    }
   ],
   "source": [
    "# For dataset generation, we need a powerful and creative model. We explicitly set the provider to gemini for this task. Ensure API Key is set correctly.\n",
    "try:\n",
    "    print(\"Loading Gemini model...\")\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        api_key = config['llm']['gemini']['gemini_api_key']\n",
    "        if api_key == \"YOUR_GEMINI_API_KEY_HERE\":\n",
    "            raise ValueError(\"Gemini API key not found. Please set it in config/config.yaml or a .env file.\")\n",
    "    model_name = config['llm']['gemini']['model_name']\n",
    "    generator_llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)\n",
    "    print(\"Successfully initialised Gemini for dataset generation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure config.yaml has 'provider: gemini' and a valid API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset generation from 13 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:32<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated 13 Q&A pairs.\n",
      "Dataset successfully saved to: ../data/generated_dataset/finetuning_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a generation prompt that instructs the LLM to act as a data creator. It needs to generate a question and a corresponding detailed answer based only on the provided text chunk.\n",
    "generation_prompt_template = \"\"\"\n",
    "You are an expert data scientist creating a dataset for instruction finetuning.\n",
    "Your task is to generate a single, high-quality question-and-answer pair based ONLY on the following text chunk.\n",
    "The question should be something a user would realistically ask about the document.\n",
    "The answer must be detailed, comprehensive, and derived exclusively from the provided text.\n",
    "\n",
    "TEXT CHUNK:\n",
    "----------\n",
    "{context}\n",
    "----------\n",
    "\n",
    "Generate the Q&A pair in the following JSON format:\n",
    "{{\n",
    "    \"question\": \"<Your generated question here>\",\n",
    "    \"answer\": \"<Your generated answer here>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "generation_prompt = PromptTemplate(\n",
    "    template=generation_prompt_template, \n",
    "    input_variables=[\"context\"]\n",
    ")\n",
    "generation_chain = generation_prompt | generator_llm | StrOutputParser()\n",
    "\n",
    "# Iterate through each document chunk, invoke the generation chain, parse the JSON output, and collect the results\n",
    "generated_data = []\n",
    "print(f\"Starting dataset generation from {len(doc_chunks)} chunks...\")\n",
    "for chunk in tqdm(doc_chunks):\n",
    "    try:\n",
    "        response_json_str = generation_chain.invoke({\"context\": chunk.page_content})\n",
    "        clean_json_str = response_json_str.strip().replace('```json', '').replace('```', '')\n",
    "        qa_pair = json.loads(clean_json_str)\n",
    "        if 'question' in qa_pair and 'answer' in qa_pair:\n",
    "            formatted_entry = {\n",
    "                \"text\": f\"<s>[INST] {qa_pair['question']} [/INST] {qa_pair['answer']} </s>\"\n",
    "            }\n",
    "            generated_data.append(formatted_entry)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"\\nWarning: Could not decode JSON from LLM response: {clean_json_str}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "print(f\"\\nSuccessfully generated {len(generated_data)} Q&A pairs.\")\n",
    "\n",
    "# Save the generated dataset to the path specified in the config\n",
    "dataset_path = \"../\"+config['finetuning']['dataset_path']\n",
    "output_dir = os.path.dirname(dataset_path)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(dataset_path, 'w') as f:\n",
    "    for entry in generated_data:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "print(f\"Dataset successfully saved to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "With our dataset created, we are now ready to proceed to the next notebook, `2_Finetuning_with_LoRA.ipynb`, where we will use this data to train our own specialised model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intellidocs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
